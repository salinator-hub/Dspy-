{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salinator-hub/Dspy-/blob/main/Script-wrappedOnboardingTask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Serve the model using vLLM"
      ],
      "metadata": {
        "id": "IoWlaISn36OH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRD5aN86v6ED",
        "outputId": "79066d84-b7c5-4eb7-eef5-b3d738cd3535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dspy-ai\n",
            "  Downloading dspy_ai-2.1.10-py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.6/152.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting vllm\n",
            "  Downloading vllm-0.3.0-cp310-cp310-manylinux1_x86_64.whl (38.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff~=2.2.1 (from dspy-ai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: joblib~=1.3.2 in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (1.3.2)\n",
            "Collecting openai<2.0.0,>=0.28.1 (from dspy-ai)\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas~=2.1.1 (from dspy-ai)\n",
            "  Downloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex~=2023.10.3 (from dspy-ai)\n",
            "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ujson~=5.8.0 (from dspy-ai)\n",
            "  Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm~=4.66.1 in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (4.66.2)\n",
            "Collecting datasets~=2.14.6 (from dspy-ai)\n",
            "  Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests~=2.31.0 in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (2.31.0)\n",
            "Collecting optuna~=3.4.0 (from dspy-ai)\n",
            "  Downloading optuna-3.4.0-py3-none-any.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from vllm)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm) (5.9.5)\n",
            "Collecting ray>=2.9 (from vllm)\n",
            "  Downloading ray-2.9.2-cp310-cp310-manylinux2014_x86_64.whl (64.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm) (0.1.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vllm) (1.25.2)\n",
            "Collecting torch==2.1.2 (from vllm)\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.37.0 (from vllm)\n",
            "  Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers==0.0.23.post1 (from vllm)\n",
            "  Downloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi (from vllm)\n",
            "  Downloading fastapi-0.109.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard] (from vllm)\n",
            "  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.6.1)\n",
            "Collecting aioprometheus[starlette] (from vllm)\n",
            "  Downloading aioprometheus-23.12.0-py3-none-any.whl (31 kB)\n",
            "Collecting pynvml==11.5.0 (from vllm)\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.2->vllm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.2->vllm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.2->vllm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2->vllm)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.2->vllm)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.2->vllm)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.2->vllm)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.2->vllm)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.2->vllm)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2->vllm)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.2->vllm)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (2.1.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->vllm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->dspy-ai) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->dspy-ai) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets~=2.14.6->dspy-ai)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->dspy-ai) (3.4.1)\n",
            "Collecting multiprocess (from datasets~=2.14.6->dspy-ai)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->dspy-ai) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->dspy-ai) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->dspy-ai) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->dspy-ai) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=0.28.1->dspy-ai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (1.3.0)\n",
            "Collecting alembic>=1.5.0 (from optuna~=3.4.0->dspy-ai)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna~=3.4.0->dspy-ai)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna~=3.4.0->dspy-ai) (2.0.27)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.1.1->dspy-ai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.1.1->dspy-ai) (2023.4)\n",
            "Collecting tzdata>=2022.1 (from pandas~=2.1.1->dspy-ai)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->vllm) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->vllm) (2.16.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (8.1.7)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.0.7)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (3.20.3)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.31.0->dspy-ai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.31.0->dspy-ai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.31.0->dspy-ai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.31.0->dspy-ai) (2024.2.2)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.0->vllm) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.0->vllm) (0.4.2)\n",
            "Collecting orjson (from aioprometheus[starlette]->vllm)\n",
            "  Downloading orjson-3.9.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting quantile-python>=1.1 (from aioprometheus[starlette]->vllm)\n",
            "  Downloading quantile-python-1.1.tar.gz (2.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting starlette>=0.14.2 (from aioprometheus[starlette]->vllm)\n",
            "  Downloading starlette-0.37.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.8 (from uvicorn[standard]->vllm)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]->vllm)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]->vllm)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]->vllm)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]->vllm)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Mako (from alembic>=1.5.0->optuna~=3.4.0->dspy-ai)\n",
            "  Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=0.28.1->dspy-ai) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.14.6->dspy-ai) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.14.6->dspy-ai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.14.6->dspy-ai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.14.6->dspy-ai) (4.0.3)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai)\n",
            "  Downloading httpcore-1.0.3-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas~=2.1.1->dspy-ai) (1.16.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna~=3.4.0->dspy-ai) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2->vllm) (2.1.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=2.9->vllm) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=2.9->vllm) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=2.9->vllm) (0.17.1)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets~=2.14.6->dspy-ai)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2->vllm) (1.3.0)\n",
            "Building wheels for collected packages: quantile-python\n",
            "  Building wheel for quantile-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for quantile-python: filename=quantile_python-1.1-py3-none-any.whl size=3444 sha256=231a7b17ac2adf7891d8da553c5cb2a2c0e44f29756966f4885e6251ceae5580\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/f4/0a/0e7d01548a005f9f3fa23101f071d248da052f2a9bf2fe11c6\n",
            "Successfully built quantile-python\n",
            "Installing collected packages: quantile-python, ninja, websockets, uvloop, ujson, tzdata, regex, python-dotenv, pynvml, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, Mako, httptools, h11, dill, colorlog, backoff, watchfiles, uvicorn, starlette, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, httpcore, alembic, aioprometheus, optuna, nvidia-cusolver-cu12, httpx, fastapi, transformers, torch, ray, openai, datasets, xformers, dspy-ai, vllm\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2023.12.25\n",
            "    Uninstalling regex-2023.12.25:\n",
            "      Successfully uninstalled regex-2023.12.25\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "bigframes 0.20.1 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.1.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.4 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.2 aioprometheus-23.12.0 alembic-1.13.1 backoff-2.2.1 colorlog-6.8.2 datasets-2.14.7 dill-0.3.7 dspy-ai-2.1.10 fastapi-0.109.2 h11-0.14.0 httpcore-1.0.3 httptools-0.6.1 httpx-0.26.0 multiprocess-0.70.15 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 openai-1.12.0 optuna-3.4.0 orjson-3.9.14 pandas-2.1.4 pynvml-11.5.0 python-dotenv-1.0.1 quantile-python-1.1 ray-2.9.2 regex-2023.10.3 starlette-0.36.3 torch-2.1.2 transformers-4.37.2 tzdata-2024.1 ujson-5.8.0 uvicorn-0.27.1 uvloop-0.19.0 vllm-0.3.0 watchfiles-0.21.0 websockets-12.0 xformers-0.0.23.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install dspy-ai vllm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run server in foreground\n",
        "# !python -m vllm.entrypoints.openai.api_server --model TheBloke/dolphin-2.6-mistral-7B-dpo-laser-AWQ --quantization awq\n",
        "\n",
        "# Run server in the background\n",
        "!nohup python -m vllm.entrypoints.openai.api_server --model TheBloke/dolphin-2.6-mistral-7B-dpo-laser-AWQ --quantization awq > server.log 2>&1 &\n",
        "# stdout is redirected to a file `server.log` using `> server.log`.\n",
        "# We use a quantized model prepared using AWQ quantization"
      ],
      "metadata": {
        "id": "07Jt1JJ4yM1a"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell again and again to monitor the status of the server.\n",
        "# The server can take a few mintues to start.\n",
        "# Once the server has started, you will see logs such as this:\n",
        "# INFO 02-10 07:16:43 llm_engine.py:877] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
        "!tail server.log"
      ],
      "metadata": {
        "id": "rdJYF7w33hR7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "877a6142-a0da-4011-9061-86dfe2163cf7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-16 13:09:35 api_server.py:209] args: Namespace(host=None, port=8000, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, root_path=None, middleware=[], model='TheBloke/dolphin-2.6-mistral-7B-dpo-laser-AWQ', tokenizer=None, revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, block_size=16, seed=0, swap_space=4, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_seqs=256, max_paddings=256, disable_log_stats=False, quantization='awq', enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\n",
            "WARNING 02-16 13:09:35 config.py:177] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
            "INFO 02-16 13:09:35 llm_engine.py:72] Initializing an LLM engine with config: model='TheBloke/dolphin-2.6-mistral-7B-dpo-laser-AWQ', tokenizer='TheBloke/dolphin-2.6-mistral-7B-dpo-laser-AWQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, seed=0)\n",
            "INFO 02-16 13:09:40 weight_utils.py:164] Using model weights format ['*.safetensors']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Once the server is up and running, this should work\n",
        "!curl http://localhost:8000/v1/models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXH6ULj6yZBF",
        "outputId": "2636dc85-8087-4107-8b1a-48ba9ea046f2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"object\":\"list\",\"data\":[{\"id\":\"TheBloke/dolphin-2.6-mistral-7B-dpo-laser-AWQ\",\"object\":\"model\",\"created\":1708089023,\"owned_by\":\"vllm\",\"root\":\"TheBloke/dolphin-2.6-mistral-7B-dpo-laser-AWQ\",\"parent\":null,\"permission\":[{\"id\":\"modelperm-63a13a7078f94726b4affa9e03dd3116\",\"object\":\"model_permission\",\"created\":1708089023,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DSPy: 𝗗eclarative 𝗦elf-improving Language 𝗣rograms"
      ],
      "metadata": {
        "id": "CgHqg5za5CZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "from dspy.evaluate import Evaluate\n",
        "from dspy.teleprompt import BayesianSignatureOptimizer, BootstrapFewShot, BootstrapFewShotWithRandomSearch, BootstrapFinetune\n"
      ],
      "metadata": {
        "id": "-k-t4RSfv-Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = dspy.HFClientVLLM(model=\"TheBloke/dolphin-2.6-mistral-7B-dpo-laser-AWQ\", port=8000, url=\"http://localhost\")\n",
        "\n",
        "dspy.settings.configure(lm=lm)\n",
        "\n",
        "colbertv2 = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
        "\n",
        "# # NOTE: After you finish this notebook, you can use GPT-3.5 like this if you like.\n",
        "# turbo = dspy.OpenAI(model='gpt-3.5-turbo-instruct')\n",
        "# # In that case, make sure to configure lm=turbo below if you choose to do that.\n",
        "\n",
        "dspy.settings.configure(rm=colbertv2)"
      ],
      "metadata": {
        "id": "YwdocZAzwCnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict = dspy.Predict('question -> answer')\n",
        "\n",
        "predict(question=\"What is the capital of  ancient India?\")"
      ],
      "metadata": {
        "id": "Qxd3VncpwSa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Emotion.py\n",
        "\n",
        "#ALL THE IMPORTS\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import dspy\n",
        "from dspy.evaluate import Evaluate\n",
        "from dspy.teleprompt import BootstrapFewShot, BootstrapFewShotWithRandomSearch, BootstrapFinetune\n",
        "from dspy.teleprompt import KNNFewShot\n",
        "from dspy.predict.knn import KNN\n",
        "import argparse\n",
        "from dspy.teleprompt import BayesianSignatureOptimizer\n",
        "\n",
        "#MODEL\n",
        "model_name=\"TheBloke/dolphin-2.6-mistral-7B-dpo-laser-AWQ\"\n",
        "lm = dspy.HFClientVLLM(model=model_name, port=8000, url=\"http://localhost\")\n",
        "dspy.settings.configure(lm=lm)\n",
        "\n",
        "# Training dataset\n",
        "train = [(\"I feel so sad when I think about the loss of my beloved pet.\", \"Sadness\"),\n",
        "         (\"This achievement brings me immense joy and satisfaction.\", \"Joy\"),\n",
        "         (\"I'm deeply in love with the beautiful scenery of this place.\", \"Love\"),\n",
        "         (\"His rude behavior towards me makes me feel anger rising inside.\", \"Anger\"),\n",
        "         (\"The thought of failing this exam fills me with fear and anxiety.\", \"Fear\"),\n",
        "         (\"What a pleasant surprise to receive unexpected gifts from friends!\", \"Surprise\"),\n",
        "         (\"Watching this heartwarming movie always brings tears to my eyes.\", \"Sadness\"),\n",
        "         (\"The success of our project fills me with pride and happiness.\", \"Joy\"),\n",
        "         (\"Being surrounded by loved ones fills my heart with warmth and love.\", \"Love\"),\n",
        "         (\"Her disrespectful attitude towards others fills me with anger.\", \"Anger\")]\n",
        "\n",
        "# Development dataset\n",
        "devset = [(\"The news of his passing brought overwhelming sadness to everyone.\", \"Sadness\"),\n",
        "       (\"The unexpected visit from my old friend filled me with joy.\", \"Joy\"),\n",
        "       (\"Expressing your love openly and honestly is the key to a happy relationship.\", \"Love\"),\n",
        "       (\"His constant complaints and negativity make me feel annoyance and anger.\", \"Anger\"),\n",
        "       (\"Walking alone in the dark alley sends shivers of fear down my spine.\", \"Fear\"),\n",
        "       (\"Finding a handwritten letter from my childhood friend was a delightful surprise.\", \"Surprise\"),\n",
        "       (\"Seeing abandoned animals on the street always fills me with sadness.\", \"Sadness\"),\n",
        "       (\"The breathtaking view from the mountaintop filled me with awe and wonder.\", \"Joy\"),\n",
        "       (\"The bond between a mother and her child is pure and unconditional love.\", \"Love\"),\n",
        "       (\"His sarcastic remarks during the meeting filled me with frustration.\", \"Anger\")]\n",
        "\n",
        "# Convert the dataset into DSPy Examples\n",
        "train = [dspy.Example(sentence=sentence, sentiment=sentiment).with_inputs('sentence') for sentence, sentiment in train]\n",
        "devset = [dspy.Example(sentence=sentence, sentiment=sentiment).with_inputs('sentence') for sentence, sentiment in devset]\n",
        "\n",
        "# Print the lengths of trainset and devset\n",
        "#print(len(train), len(devset))\n",
        "\n",
        "# Access an example from trainset and devset\n",
        "train_example = train[0]\n",
        "dev_example = devset[0]\n",
        "# print(train_example.sentence)\n",
        "\n",
        "\n",
        "\n",
        "def should_be_correct(sen, set):\n",
        "  check=dspy.Predict(\"sentence -> sentiment\")\n",
        "  result=check(sentence=f\"Is this {set}; a correct sentiment for {sen}. Return 0 for False and 1 for True.\")\n",
        "  if result.sentiment==0:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "class Emotion(dspy.Signature):\n",
        "    \"\"\"Classify emotion among sadness, joy, love, anger, fear, surprise.\"\"\"\n",
        "\n",
        "    sentence = dspy.InputField(desc = \"Input sentence\")\n",
        "    sentiment = dspy.OutputField(desc = \"Predicted Emotion in word\")\n",
        "\n",
        "\n",
        "class EmotionPipelineAssertions(dspy.Module):\n",
        "    def __init__(self, assert_on=True):\n",
        "        super().__init__()\n",
        "        self.assert_on = assert_on\n",
        "        self.Emotion_predicted = dspy.ChainOfThought(Emotion)\n",
        "\n",
        "    def forward(self, sentence, **kwargs):\n",
        "        predicted_Sentiment = self.Emotion_predicted(sentence=sentence)\n",
        "\n",
        "        if self.assert_on:\n",
        "            dspy.Suggest(\n",
        "                should_be_correct(sentence, predicted_Sentiment),\n",
        "                \"Emotion is correct.\",\n",
        "                target_module=Emotion\n",
        "            )\n",
        "\n",
        "        return predicted_Sentiment\n",
        "\n",
        "\n",
        "class Assess(dspy.Signature):\n",
        "    \"\"\"Assess the quality of the solution along the specified dimension.\"\"\"\n",
        "\n",
        "    assessed_text = dspy.InputField()\n",
        "    assessment_sentence = dspy.InputField()\n",
        "    assessment_sentiment = dspy.OutputField(desc=\"Only True or False\")\n",
        "\n",
        "\n",
        "def metric(gold, pred, trace=None):\n",
        "    sentence, sentiment, emos = gold.sentence, gold.sentiment, pred.sentiment\n",
        "\n",
        "    correctness = f\"The given emos{emos} will be effective for this sentence {sentence}. The gold sentiment is {sentiment}\"\n",
        "    informative = f\"For this sentence {sentence} is the given {emos} informative. The gold sentiment is {sentiment}\"\n",
        "\n",
        "    correct = dspy.ChainOfThought(Assess)(assessed_text=sentence, assessment_sentence=correctness)\n",
        "    informative = dspy.ChainOfThought(Assess)(assessed_text=sentence, assessment_sentence=informative)\n",
        "\n",
        "    correctness_score = correct.assessment_sentiment\n",
        "    info_score = informative.assessment_sentiment\n",
        "    score = 0\n",
        "\n",
        "    if correctness_score == \"True\":\n",
        "        score += 1\n",
        "\n",
        "    if info_score == \"True\":\n",
        "        score += 1\n",
        "\n",
        "    return score / 2.0\n",
        "\n",
        "\n",
        "# Define your devset and kwargs here\n",
        "# Replace placeholders with your actual development dataset and any additional keyword arguments you need for evaluation\n",
        "\n",
        "kwargs = dict(display_progress=True, display_table=5)\n",
        "\n",
        "# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\n",
        "evaluate_on_hotpotqa = Evaluate( devset = devset, num_threads=1, display_progress=True, display_table=5)\n",
        "\n",
        "# Define function to evaluate pipeline\n",
        "def evaluate_pipeline(model_name, assertion=True, bayesian=True):\n",
        "    Emotion_predicted = EmotionPipelineAssertions()\n",
        "\n",
        "    # Compile the pipeline with specified configurations\n",
        "    if assertion and bayesian:\n",
        "        compiled_pipeline = BayesianSignatureOptimizer(metric=metric, n=5, init_temperature=1.0).compile(Emotion_predicted, devset=devset, optuna_trials_num=5, max_bootstrapped_demos=3, max_labeled_demos=5, eval_kwargs=kwargs)\n",
        "        config_label = \"Optimized Pipeline\"\n",
        "    elif assertion and not bayesian:\n",
        "        compiled_pipeline = Emotion_predicted\n",
        "        config_label = \"Without Bayesian\"\n",
        "    elif not assertion and bayesian:\n",
        "        without_assertion=EmotionPipelineAssertions(assert_on=False)\n",
        "        compiled_pipeline = BayesianSignatureOptimizer(metric=metric, n=5, init_temperature=1.0).compile(without_assertion, devset=devset, optuna_trials_num=5, max_bootstrapped_demos=3, max_labeled_demos=5, eval_kwargs=kwargs)\n",
        "        config_label = \"Without Assertion\"\n",
        "    else:\n",
        "        without_assertion_and_bayesian=EmotionPipelineAssertions(assert_on=False)\n",
        "        compiled_pipeline = without_assertion_and_bayesian\n",
        "        config_label = \"No Assertion and No Bayesian\"\n",
        "\n",
        "    # Evaluate the compiled pipeline\n",
        "    eval_score = evaluate_on_hotpotqa(compiled_pipeline, metric=metric)\n",
        "    return eval_score, config_label\n",
        "\n",
        "\n",
        "# Run the evaluation for each configuration\n",
        "def run_ablation_study(model_name, disable_assertion=False, disable_bayesian=False):\n",
        "    scores = []\n",
        "    config_labels = []\n",
        "\n",
        "    for assertion in [True, False]:\n",
        "        if disable_assertion and not assertion:\n",
        "            continue\n",
        "        for bayesian in [True, False]:\n",
        "            if disable_bayesian and not bayesian:\n",
        "                continue\n",
        "            eval_score, config_label = evaluate_pipeline(model_name, assertion=assertion, bayesian=bayesian)\n",
        "            scores.append(eval_score)\n",
        "            config_labels.append(config_label)\n",
        "\n",
        "    return scores, config_labels\n",
        "\n",
        "def use_pipeline_as_user(model_name, sentence):\n",
        "    Emotion_predicted = EmotionPipelineAssertions()\n",
        "    result = Emotion_predicted(sentence=sentence)\n",
        "    print(\"Predicted Emotion:\", result.sentiment)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Pipeline for suggesting home remedies.\")\n",
        "    parser.add_argument(\"--assertion\", type=bool, default=True, help=\"Whether to include assertion in the pipeline. Default: True\")\n",
        "    parser.add_argument(\"--bayesian\", type=bool, default=True, help=\"Whether to use Bayesian optimization. Default: True\")\n",
        "    parser.add_argument(\"--run-ablation\", action=\"store_true\", help=\"Perform an ablation study.\")\n",
        "    parser.add_argument(\"--model-name\", type=str, default=\"TheBloke/dolphin-2.6-mistral-7B-dpo-laser-AWQ\", help=\"Name of the language model. Default: TheBloke/dolphin-2.6-mistral-7B-dpo-laser-AWQ\")\n",
        "    parser.add_argument(\"--user-question\", type=str, help=\"Question to use the pipeline as a user.\")\n",
        "    parser.add_argument(\"--disable-assertion\", action=\"store_true\", help=\"Disable assertion in the pipeline during ablation study.\")\n",
        "    parser.add_argument(\"--disable-bayesian\", action=\"store_true\", help=\"Disable Bayesian optimization in the pipeline during ablation study.\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.run_ablation:\n",
        "        scores, config_labels = run_ablation_study(args.model_name, disable_assertion=args.disable_assertion, disable_bayesian=args.disable_bayesian)\n",
        "        # Plotting the results\n",
        "        x = np.arange(len(config_labels))\n",
        "        plt.bar(x, scores)\n",
        "        plt.xlabel('Configuration')\n",
        "        plt.ylabel('Evaluation Score')\n",
        "        plt.title('Ablation Study Results')\n",
        "        plt.xticks(x, config_labels, rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    elif args.user_sentence:\n",
        "        use_pipeline_as_user(args.model_name, args.user_sentence)\n",
        "    else:\n",
        "        eval_score, config_label = evaluate_pipeline(args.model_name, assertion=args.assertion, bayesian=args.bayesian)\n",
        "        print(f\"Evaluation Score: {eval_score}, Configuration: {config_label}\")\n"
      ],
      "metadata": {
        "id": "RV7hdwgz7M2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870f6282-793f-4ecf-fcf2-63c595885ad6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Emotion.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python Emotion.py --run-ablation --disable-assertion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux0tRzCQA-cz",
        "outputId": "66b663af-5b75-47b9-f25b-90baf6c5b63f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 30% 3/10 [00:40<01:34, 13.53s/it]\n",
            "Bootstrapped 3 full traces after 4 examples in round 0.\n",
            " 30% 3/10 [00:42<01:38, 14.13s/it]\n",
            "Bootstrapped 3 full traces after 4 examples in round 0.\n",
            " 30% 3/10 [00:39<01:33, 13.32s/it]\n",
            "Bootstrapped 3 full traces after 4 examples in round 0.\n",
            " 30% 3/10 [00:42<01:38, 14.04s/it]\n",
            "Bootstrapped 3 full traces after 4 examples in round 0.\n",
            "\u001b[32m[I 2024-02-16 14:04:59,923]\u001b[0m A new study created in memory with name: no-name-03467153-6b0b-4d7e-8cde-83ff2c82e7de\u001b[0m\n",
            "Average Metric: 8.5 / 10  (85.0): 100% 10/10 [02:13<00:00, 13.32s/it]\n",
            "Average Metric: 8.5 / 10  (85.0%)\n",
            "/usr/local/lib/python3.10/dist-packages/dspy/evaluate/evaluate.py:142: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(truncate_cell)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Assess the quality of the solution along the specified dimension.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Assessed Text: ${assessed_text}\n",
            "\n",
            "Assessment Sentence: ${assessment_sentence}\n",
            "\n",
            "Reasoning: Let's think step by step in order to ${produce the assessment_sentiment}. We ...\n",
            "\n",
            "Assessment Sentiment: Only True or False\n",
            "\n",
            "---\n",
            "\n",
            "Assessed Text: His sarcastic remarks during the meeting filled me with frustration.\n",
            "\n",
            "Assessment Sentence: For this sentence His sarcastic remarks during the meeting filled me with frustration. is the given Anger --- Sentence: The unexpected compliment from my boss made me feel happy and joyful. Reasoning: Let's think step by step in order to ${produce the sentiment}. We ... - Recall the unexpected compliment - Feel the happiness and joy of being appreciated - Understand that the situation is a positive one Given: \"The text in consideration is: ... Emotion: ... Joy --- Sentence: The informative. The gold sentiment is Anger\n",
            "\n",
            "Reasoning: Let's think step by step in order to\u001b[32m ${produce the sentiment}. We ... - Recall the sarcastic remarks - Feel the frustration of being mocked - Understand that the situation is a negative one Given: \"The text in consideration is: His sarcastic remarks during the meeting filled me with frustration. Emotion: ... Anger --- Sentence: His sarcastic remarks during the meeting filled me with frustration.\n",
            "\n",
            "Assessment Sentiment: True\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\u001b[32m[I 2024-02-16 14:07:13,128]\u001b[0m Trial 0 finished with value: 85.0 and parameters: {'133473254101952_predictor_instruction': 1, '133473254101952_predictor_demos': 4}. Best is trial 0 with value: 85.0.\u001b[0m\n",
            "Average Metric: 10.0 / 10  (100.0): 100% 10/10 [02:20<00:00, 14.10s/it]\n",
            "Average Metric: 10.0 / 10  (100.0%)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Assess the quality of the solution along the specified dimension.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Assessed Text: ${assessed_text}\n",
            "\n",
            "Assessment Sentence: ${assessment_sentence}\n",
            "\n",
            "Reasoning: Let's think step by step in order to ${produce the assessment_sentiment}. We ...\n",
            "\n",
            "Assessment Sentiment: Only True or False\n",
            "\n",
            "---\n",
            "\n",
            "Assessed Text: His sarcastic remarks during the meeting filled me with frustration.\n",
            "\n",
            "Assessment Sentence: For this sentence His sarcastic remarks during the meeting filled me with frustration. is the given Anger --- Sentence: The thought of spending the rest of my life with him fills me with love and happiness. Consider the range of human emotions – sadness, joy informative. The gold sentiment is Anger\n",
            "\n",
            "Reasoning: Let's think step by step in order to assess the quality of the solution along the specified dimension. The dimension here is the emotion expressed in the sentence. The sentence \"His sarcastic remarks during the meeting filled me with frustration.\" clearly expresses the emotion of frustration. Therefore, the quality of the solution along the specified dimension is True.\n",
            "\n",
            "Assessment Sentiment:\u001b[32m True\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\u001b[32m[I 2024-02-16 14:09:34,131]\u001b[0m Trial 1 finished with value: 100.0 and parameters: {'133473254101952_predictor_instruction': 3, '133473254101952_predictor_demos': 2}. Best is trial 1 with value: 100.0.\u001b[0m\n",
            "Average Metric: 10.0 / 10  (100.0): 100% 10/10 [01:35<00:00,  9.59s/it]\n",
            "Average Metric: 10.0 / 10  (100.0%)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Assess the quality of the solution along the specified dimension.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Assessed Text: ${assessed_text}\n",
            "\n",
            "Assessment Sentence: ${assessment_sentence}\n",
            "\n",
            "Reasoning: Let's think step by step in order to ${produce the assessment_sentiment}. We ...\n",
            "\n",
            "Assessment Sentiment: Only True or False\n",
            "\n",
            "---\n",
            "\n",
            "Assessed Text: His sarcastic remarks during the meeting filled me with frustration.\n",
            "\n",
            "Assessment Sentence: For this sentence His sarcastic remarks during the meeting filled me with frustration. is the given Anger informative. The gold sentiment is Anger\n",
            "\n",
            "Reasoning: Let's think step by step in order to\u001b[32m assess the quality of the solution along the specified dimension. The sentence is about someone's sarcastic remarks during a meeting, and it is clear that the speaker is expressing frustration. The gold sentiment is Anger, and the sentence does convey this emotion. Therefore, the assessment is that the sentence is informative about the specified dimension (Anger).\n",
            "\n",
            "Assessment Sentiment: True\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\u001b[32m[I 2024-02-16 14:11:10,017]\u001b[0m Trial 2 finished with value: 100.0 and parameters: {'133473254101952_predictor_instruction': 0, '133473254101952_predictor_demos': 0}. Best is trial 1 with value: 100.0.\u001b[0m\n",
            "Average Metric: 8.0 / 10  (80.0): 100% 10/10 [02:29<00:00, 14.91s/it]\n",
            "Average Metric: 8.0 / 10  (80.0%)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Assess the quality of the solution along the specified dimension.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Assessed Text: ${assessed_text}\n",
            "\n",
            "Assessment Sentence: ${assessment_sentence}\n",
            "\n",
            "Reasoning: Let's think step by step in order to ${produce the assessment_sentiment}. We ...\n",
            "\n",
            "Assessment Sentiment: Only True or False\n",
            "\n",
            "---\n",
            "\n",
            "Assessed Text: His sarcastic remarks during the meeting filled me with frustration.\n",
            "\n",
            "Assessment Sentence: For this sentence His sarcastic remarks during the meeting filled me with frustration. is the given Anger --- Sentence: The unexpected compliment from my boss made me feel happy and joyful. Sentiment: Joy --- Sentence: The thought of losing my job makes me feel fear and anxiety. Sentiment: Fear --- Sentence: The bond between a mother and her child is pure and unconditional love. Sentiment: Love --- Sentence: The unexpected compliment from my boss made me feel happy and joyful. Sent informative. The gold sentiment is Anger\n",
            "\n",
            "Reasoning: Let's think step by step in order to assess the quality of the solution along the specified dimension. The sentence \"His sarcastic remarks during the meeting filled me with frustration.\" is expressing a feeling of frustration. The dimension specified is Anger. Frustration is a form of anger, so the quality of the solution along the specified dimension is True.\n",
            "\n",
            "Assessment Sentiment:\u001b[32m True\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\u001b[32m[I 2024-02-16 14:13:39,170]\u001b[0m Trial 3 finished with value: 80.0 and parameters: {'133473254101952_predictor_instruction': 0, '133473254101952_predictor_demos': 4}. Best is trial 1 with value: 100.0.\u001b[0m\n",
            "Average Metric: 10.0 / 10  (100.0): 100% 10/10 [02:29<00:00, 14.93s/it]\n",
            "Average Metric: 10.0 / 10  (100.0%)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Assess the quality of the solution along the specified dimension.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Assessed Text: ${assessed_text}\n",
            "\n",
            "Assessment Sentence: ${assessment_sentence}\n",
            "\n",
            "Reasoning: Let's think step by step in order to ${produce the assessment_sentiment}. We ...\n",
            "\n",
            "Assessment Sentiment: Only True or False\n",
            "\n",
            "---\n",
            "\n",
            "Assessed Text: His sarcastic remarks during the meeting filled me with frustration.\n",
            "\n",
            "Assessment Sentence: For this sentence His sarcastic remarks during the meeting filled me with frustration. is the given Frustration --- Sentence: The thought informative. The gold sentiment is Anger\n",
            "\n",
            "Reasoning: Let's think step by step in order to assess the quality of the solution along the specified dimension. The dimension here is the emotion expressed in the sentence. The sentence \"His sarcastic remarks during the meeting filled me with frustration.\" clearly expresses the emotion of frustration. Therefore, the gold sentiment is indeed Anger. However, the gold sentiment is not \"Informative\" as it does not describe the nature of the information provided in the sentence. The gold sentiment is \"Anger\" because it describes the emotion experienced by the speaker due to the sarcastic remarks.\n",
            "\n",
            "Assessment Sentiment:\u001b[32m True\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\u001b[32m[I 2024-02-16 14:16:08,487]\u001b[0m Trial 4 finished with value: 100.0 and parameters: {'133473254101952_predictor_instruction': 3, '133473254101952_predictor_demos': 3}. Best is trial 1 with value: 100.0.\u001b[0m\n",
            "Average Metric: 10.0 / 10  (100.0): 100% 10/10 [00:00<00:00, 166.92it/s]\n",
            "Average Metric: 10.0 / 10  (100.0%)\n",
            "<pandas.io.formats.style.Styler object at 0x7964a9e5feb0>\n",
            "<IPython.core.display.HTML object>\n",
            "Average Metric: 10.0 / 10  (100.0): 100% 10/10 [00:00<00:00, 208.59it/s]\n",
            "Average Metric: 10.0 / 10  (100.0%)\n",
            "/usr/local/lib/python3.10/dist-packages/dspy/evaluate/evaluate.py:142: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(truncate_cell)\n",
            "<pandas.io.formats.style.Styler object at 0x7964a94d5f00>\n",
            "<IPython.core.display.HTML object>\n",
            "Figure(640x480)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python Emotion.py --user_sentence \"are you ok\""
      ],
      "metadata": {
        "id": "JB2A__U0gO1t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}